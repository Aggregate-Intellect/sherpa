Building ResearchLLM: automated statistical research and interpretation
=======================================================================

ResearchGPT is a natural language statistics agent. Users provide the
agent with a data set and natural language queries. The agent writes
code to answer their questions and provide interpretations based on its
analysis. Raw data is never shared with the LLM itself, and generated
code is run locally. You can see a demo video
`here <https://phasellm.com/researchgpt>`__. This session will cover the
underlying architecture of ResearchGPT and how it’s been tested using
PhaseLLM, a developer tooling framework for testing and robustifying
LLM-powered apps.

`SLIDES <#>`__ \| `RECORDING <https://youtu.be/yqmLF3a9aLM>`__

**SUMMARY**

-  ResearchGPT is a demo product and project of a broader package called
   PhaseLLM, which is essentially a tool to help build more robust,
   large language model-powered apps.
-  There are three different models preloaded in the demo - GPT4, GPT
   3.5, and Claude.
-  The model can generate Python code that can be used for data
   analysis.
-  The model is running locally and not sharing data with any of the
   large language models themselves.
-  The model can be used to find correlations between different
   variables, both numerical and categorical.
-  A future direction is creating a workflow where the model builds the
   whole project and even searches the web for results.
-  You can think of it as a data science assistant.
-  The model is given a specific prompt to act in a predictable way.
-  The data is loaded into a local server and described using functions.
-  The model is asked to generate Python code for data analysis.
-  The code is run locally and errors are tracked using specific
   exception classes.
-  A chat bot class is built to abstract the process for different
   models.
-  Sandbox functionality is being developed for public availability.
-  The chatbot resubmits prompts with updated instructions to make the
   model desensitized to specific types of errors.
-  The code output is interpreted to provide an analysis of the data.
-  LLMs offer opportunities to build cool apps without much knowledge of
   deep learning, but understanding which models to use in specific
   scenarios is crucial.
-  Non-deterministic errors such as how the model interprets results
   also need to be considered
-  There are still issues with LLMs, such as unterminated string
   literals, but overall the code generated by LLMs is impressive.
-  The speaker discusses a new approach called chain of thought
   reasoning, which generates a markdown file with analysis steps and
   code for each step.
-  The speaker is excited about the potential for LLMs to automate
   research and create new knowledge.
-  They mention that there are plans to incorporate other agents into
   the system and allow for data searching and data set creation.
-  There is discussion on benchmarking LLM performance, with speed of
   execution and quality of code being important factors.
-  The speaker discusses the importance of requesting specific types of
   code output from LLMs and guiding them towards certain libraries.
-  They mention the need to avoid visualizing data and instead focus on
   providing output that can be easily interpreted and evaluated by
   humans.
-  The speaker also touches upon the importance of testing and
   evaluating LLM outputs, and discusses a workflow that involves having
   models evaluate each other’s interpretations.
-  The topic of not sending sensitive data to large language models is
   discussed, and the speaker provides details on how they refrain from
   sending sensitive data by only sending specific prompts and guiding
   the models towards certain types of outputs.
-  The speakers discuss the potential application of LLMs for creating
   small models that are trained on company data, which can provide an
   aggregate answer to queries without risking the exposure of sensitive
   data.
-  The focus of PhaseLLM is on robustness and user experience, with an
   emphasis on dealing with exceptions and testing.
-  The speakers mention the potential for collaboration between multiple
   models and the inclusion of a filtering layer to help with
   de-identification and data structuring.
-  The feedback from customer discovery interviews suggests that people
   are excited about the potential of LLMs for various applications
-  The speakers discuss the potential for open-core software and
   enterprise-level engineered products to be created around LLMs for
   various use cases.
-  The importance of controlling the behavior of the models and the need
   for robustness is emphasized.
-  Data visualization and merging datasets are identified as potential
   new features for LLMs.
-  The possibility of using LLMs to edit Excel files is brought up as a
   potential new feature to explore.
-  Different companies may have varying comfort levels with using open
   AI depending on their data retention policies.
-  The possibility of having different cloud solutions and open core
   solutions for LLMs is discussed, with companies selecting the ones
   that are right for them.

**Wojciech Gryc (Founder @ Phase AI)**

`Wojciech <https://www.linkedin.com/in/wojciechgryc>`__ is the
co-founder and CEO of Phase AI, where he helps startups and scaleups
launch AI-driven products. He is the developer behind PhaseLLM and
ResearchGPT. Wojciech started his career as an AI researcher at IBM
Research, and completed his graduate studies at the Oxford Internet
Institute. Prior to Phase AI, he was the founder and CEO of Canopy Labs,
a customer data platform funded by Y Combinator and acquired by Drop.

.. image:: ../_imgs/wojciechg.jpeg
  :width: 400
  :alt: Wojciech Gryc Headshot