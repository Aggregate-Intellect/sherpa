Commercializing LLMs: Lessons and Ideas for Agile Innovation
============================================================

In this talk, Josh, an ML expert with experience commercializing
NLP-powered services, will discuss the potential for leveraging
foundation models to drive agile innovation in both individual and
organizational processes. He will share lessons learned from his work
with a bootstrapped startup and provide insights on how LLMs can be
commercialized effectively.

`SLIDES <https://github.com/Aggregate-Intellect/sherpa/blob/main/KnowledgeOps/Commercialization%20Strategy%20with%20LLMs.pdf>`__
\| `RECORDING <https://youtu.be/QfX648IZg3U>`__

**SUMMARY**

-  Opportunities for large language models in startups and innovation

   -  1/21: Large language models offer exciting possibilities for
      startups and product development. They can revolutionize the
      innovation process with qualitatively new ways of problem-solving.
      Before building a product, take a step back and see if an approach
      powered by these models could serve a purpose.
   -  2/21: GitHub co-pilot is a perfect example of how large language
      models can be used in engineering. By offering new ways of writing
      code, it streamlines the development process and allows engineers
      to focus on solving problems.
   -  3/21: Quick feedback from users is crucial for product
      development, even if you are one of the target users. Hackathons
      and accelerators can be beneficial to startups by providing access
      to large language models as a service and infrastructure credits.
   -  4/21: Building a lean, agile startup focused on R&D is aided by
      resources such as infrastructure credits, tax credits, subsidies,
      marketing grants, and IP support. In Canada, AI startups receive
      federal and provincial programs supporting R&D. Bootstrapping can
      sustain a company from day one.
   -  5/21: Bootstrapping can be a good way to sustain a company based
      on revenue from day one. It also forces you to start selling as
      soon as possible to get revenue and establish quick feedback for
      market fit. Investment is not always necessary for R&D.

-  IP strategy for large language models

   -  6/21: When building large language models for commercialization,
      it’s important to consider IP strategy and seek advice from
      experts.
   -  7/21: Applying large language models to improve existing
      capabilities may not be patentable, but rethinking the entire
      approach to solve a problem in a new way is more likely to be
      patentable. #MLdevelopment #IPstrategy

-  Large language models for R&D

   -  8/21: During R&D, specify tasks, models to optimize, & create
      dataset to evaluate models against. Use large language models for
      weak label generation & data augmentation to make data set
      curation easier. #AI #ML #R&D
   -  9/21: R&D conversations are often open & difficult to structure,
      but they can still be turned into components used by large
      language models. For example, to create a conversational bot for
      Slack, modularize into 3 LLM calls for message, trigger, &
      audience classification. #AI #ML #chatbot
   -  10/21: With modularization & templated code repository, create
      conversational bot very quickly. Use LLM-generated info for
      message content classification, trigger classification, & audience
      classification. #AI #ML #chatbot #Slack

-  Microservices and modularization of products based on large language
   models

   -  11/21: Large language models (LLMs) can be used in various stages
      of a project, including engineering and production. They can be
      leveraged to build microservices or components that work together
      to produce the desired output.
   -  12/21: Treating a LLM as one of the microservices of a product
      enables breaking down the problem into smaller and more manageable
      pieces. This allows for easier implementation and development,
      additional R&D, unit testing, and quality control.
   -  13/21: This approach also allows for easier explainability and
      optimization of the process. The components in the architecture of
      a system that leverages LLMs should interface the language skills
      of GPT-like models and domain-specific expertise to get the best
      results.
   -  14/21: In our slack app example, the LLM prompts used depend on
      the context and specifics of what is being done. In some cases,
      having a good prompt engineer is crucial, while in others, we can
      just pass exemplars that would have a bigger influence on the
      performance.
   -  15/21: We can add a dialogue analysis component to control the
      prompts generated by LLMs in a particular domain. This component
      can infer information necessary to determine the most relevant
      examples when generating probing questions.
   -  16/21: Another component can create dynamic prompts for in-context
      learning by choosing the best exemplars from a repository or by
      retrieving documents containing domain knowledge. This can
      significantly improve the performance of LLMs.
   -  17/21: Finally, a quality control component can rank generated
      candidates in case the LLM generates a question or answer that is
      not suitable. Rule-based and human-crafted questions generated
      using question recipes can also be used to ensure that
      inappropriate responses are avoided.

-  Production stage of large language models

   -  18/21: In production, large language models can be optimized by
      combining them with other components to create an ecosystem of
      microservices that work together. This approach ensures better
      performance and improved efficiency of the overall system.
   -  19/21: When optimizing costs during a project, evaluating both
      performance and cost is crucial. If a cheaper model performs
      almost as well as a more expensive one, it’s advisable to choose
      the former. This way, you can save money without compromising on
      performance.
   -  20/21: After some data is collected and the overall system
      performance is more well established, it’s sometimes possible to
      replace large language models with cheaper and leaner models in
      production. This approach helps to reduce costs without
      sacrificing performance.
   -  21/21: When building a startup with large language models, it’s
      essential to consider the service level availability uptime of
      cloud infrastructure. Microsoft Azure Open AI service offers 99.9%
      uptime, ensuring that your system will be available to users when
      they need it.

*Resources*

-  `Which Model Shall I Choose? Cost/Quality Trade-offs for Text
   Classification Tasks <https://arxiv.org/abs/2301.07006>`__
-  `Canada Government Business Benefits
   Finder <https://innovation.ised-isde.canada.ca/innovation/s/?language=en_CA>`__
-  `Understanding UK Artificial Intelligence R&D commercialisation and
   the role of
   standards <https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1079959/DCMS_and_OAI_-_Understanding_UK_Artificial_Intelligence_R_D_commercialisation__accessible.pdf>`__

----

**Josh Seltzer (CTO @ Nexxt Intelligence)**

`Josh <https://www.linkedin.com/in/josh-seltzer/>`__ is the CTO at Nexxt
Intelligence, where he leads R&D on LLMs and NLP to build innovative
solutions for the market research industry. He also works in
biodiversity and applications of AI for conservation.

.. image:: ../_imgs/joshs.png
  :width: 400
  :alt: Josh Seltzer Headshot